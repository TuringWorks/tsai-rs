{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Efficiently Work with Very Large Numpy Arrays\n",
    "\n",
    "This notebook demonstrates how to work with large numpy arrays that don't fit in memory using **tsai-rs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Sometimes datasets are really big (many GBs) and don't fit in memory. The solution is to use **np.memmap** - memory-mapped arrays that work like arrays on disk.\n",
    "\n",
    "### Key Benefits\n",
    "- Access small segments of large files without loading entire file into memory\n",
    "- Fast batch creation for deep learning\n",
    "- Similar to how image files work - you pass paths, then load on demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Large Arrays on Disk\n",
    "\n",
    "For demonstration, we'll create arrays on disk using chunks that fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data directory\n",
    "path = Path('./data')\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a moderately large array for demonstration\n",
    "# In practice, you might have 10GB+ arrays\n",
    "n_samples = 10000\n",
    "n_vars = 50\n",
    "seq_len = 512\n",
    "\n",
    "print(f\"Creating array with shape: ({n_samples}, {n_vars}, {seq_len})\")\n",
    "print(f\"Estimated size: {n_samples * n_vars * seq_len * 4 / 1e9:.2f} GB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array in chunks\n",
    "def create_large_array_on_disk(path, shape, dtype=np.float32, chunksize=1000):\n",
    "    \"\"\"Create a large array on disk in chunks.\"\"\"\n",
    "    # Create empty memmap file\n",
    "    fp = np.memmap(path, dtype=dtype, mode='w+', shape=shape)\n",
    "    \n",
    "    # Fill in chunks\n",
    "    n_samples = shape[0]\n",
    "    for start in range(0, n_samples, chunksize):\n",
    "        end = min(start + chunksize, n_samples)\n",
    "        fp[start:end] = np.random.randn(end - start, *shape[1:]).astype(dtype)\n",
    "        if start % 5000 == 0:\n",
    "            print(f\"  Progress: {start}/{n_samples}\")\n",
    "    \n",
    "    fp.flush()  # Ensure data is written to disk\n",
    "    del fp  # Close the memmap\n",
    "    print(f\"Created array at {path}\")\n",
    "\n",
    "# Uncomment to create the array (takes a few seconds)\n",
    "# create_large_array_on_disk(path/'X_large.npy', (n_samples, n_vars, seq_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Arrays as Memmaps\n",
    "\n",
    "The key is to use `mmap_mode` when loading with `np.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small demo array\n",
    "demo_array = np.random.randn(1000, 10, 100).astype(np.float32)\n",
    "np.save(path/'X_demo.npy', demo_array)\n",
    "\n",
    "print(f\"Saved demo array: {demo_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as memmap (mode 'c' = copy-on-write)\n",
    "X_on_disk = np.load(path/'X_demo.npy', mmap_mode='c')\n",
    "\n",
    "print(f\"Loaded memmap: {X_on_disk.shape}\")\n",
    "print(f\"Type: {type(X_on_disk)}\")\n",
    "\n",
    "# Key insight: very little RAM is used!\n",
    "print(f\"\\nMemory used by memmap object: {sys.getsizeof(X_on_disk)} bytes\")\n",
    "print(f\"(Actual data size: {X_on_disk.nbytes} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmap Modes\n",
    "\n",
    "| Mode | Description |\n",
    "|------|-------------|\n",
    "| 'r' | Read-only |\n",
    "| 'r+' | Read and write (changes saved to disk) |\n",
    "| 'w+' | Create/overwrite for read/write |\n",
    "| 'c' | Copy-on-write (changes in memory only) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode 'c' allows modifications in memory without changing disk\n",
    "# This is ideal for data augmentation during training\n",
    "\n",
    "sample = X_on_disk[0].copy()  # Get a sample\n",
    "sample_modified = sample + 1  # Modify in memory\n",
    "\n",
    "print(\"Original disk data unchanged:\")\n",
    "print(f\"  X_on_disk[0, 0, 0] = {X_on_disk[0, 0, 0]:.4f}\")\n",
    "print(f\"  Modified sample[0, 0] = {sample_modified[0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Slicing is very fast\n",
    "def benchmark_slicing(arr, n_trials=1000):\n",
    "    \"\"\"Benchmark slicing speed.\"\"\"\n",
    "    start = time.time()\n",
    "    for i in range(n_trials):\n",
    "        _ = arr[i % len(arr)]\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed / n_trials * 1000  # ms per slice\n",
    "\n",
    "time_per_slice = benchmark_slicing(X_on_disk)\n",
    "print(f\"Time per slice: {time_per_slice:.4f} ms\")\n",
    "print(f\"Slices per second: {1000/time_per_slice:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing along different axes\n",
    "print(\"Slicing examples:\")\n",
    "print(f\"  X[0] shape: {X_on_disk[0].shape}\")  # One sample\n",
    "print(f\"  X[:, 0] shape: {X_on_disk[:, 0].shape}\")  # One variable\n",
    "print(f\"  X[:, :, 0] shape: {X_on_disk[:, :, 0].shape}\")  # One time step\n",
    "print(f\"  X[:10] shape: {X_on_disk[:10].shape}\")  # Batch of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with tsai-rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "n_classes = 5\n",
    "y = np.random.randint(0, n_classes, size=len(X_on_disk))\n",
    "\n",
    "# Standardize the memmap (works on slices)\n",
    "# For very large arrays, you might standardize batch by batch\n",
    "X_std = tsai_rs.ts_standardize(X_on_disk[:100].astype(np.float32), by_sample=True)\n",
    "\n",
    "print(f\"Standardized batch shape: {X_std.shape}\")\n",
    "print(f\"Mean: {X_std.mean():.4f}, Std: {X_std.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TSDataset from a batch\n",
    "batch_size = 64\n",
    "X_batch = X_on_disk[:batch_size].astype(np.float32)\n",
    "y_batch = y[:batch_size]\n",
    "\n",
    "# Standardize\n",
    "X_batch_std = tsai_rs.ts_standardize(X_batch, by_sample=True)\n",
    "\n",
    "# Create dataset\n",
    "ds = tsai_rs.TSDataset(X_batch_std, y_batch)\n",
    "print(f\"Created dataset: {ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_memmap, y, batch_size=32, shuffle=True):\n",
    "    \"\"\"Generate batches from memmap array.\"\"\"\n",
    "    n_samples = len(X_memmap)\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_idx = indices[start:end]\n",
    "        \n",
    "        # Load batch from memmap\n",
    "        X_batch = np.array([X_memmap[i] for i in batch_idx]).astype(np.float32)\n",
    "        y_batch = y[batch_idx]\n",
    "        \n",
    "        # Standardize\n",
    "        X_batch = tsai_rs.ts_standardize(X_batch, by_sample=True)\n",
    "        \n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Example usage\n",
    "for i, (X_batch, y_batch) in enumerate(batch_generator(X_on_disk, y, batch_size=32)):\n",
    "    if i == 0:\n",
    "        print(f\"Batch 0: X shape = {X_batch.shape}, y shape = {y_batch.shape}\")\n",
    "    if i >= 2:\n",
    "        break\n",
    "print(f\"Generated {i+1} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_efficient_pipeline(X_path, y, n_epochs=2, batch_size=32):\n",
    "    \"\"\"Complete memory-efficient training pipeline.\"\"\"\n",
    "    \n",
    "    # Load as memmap\n",
    "    X_memmap = np.load(X_path, mmap_mode='c')\n",
    "    print(f\"Loaded memmap: {X_memmap.shape}\")\n",
    "    print(f\"RAM used by memmap: ~{sys.getsizeof(X_memmap)} bytes\")\n",
    "    \n",
    "    n_vars = X_memmap.shape[1]\n",
    "    seq_len = X_memmap.shape[2]\n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    # Configure model\n",
    "    config = tsai_rs.InceptionTimePlusConfig(\n",
    "        n_vars=n_vars,\n",
    "        seq_len=seq_len,\n",
    "        n_classes=n_classes\n",
    "    )\n",
    "    print(f\"Model config: {config}\")\n",
    "    \n",
    "    # Training loop\n",
    "    n_batches_per_epoch = len(X_memmap) // batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
    "        \n",
    "        for i, (X_batch, y_batch) in enumerate(batch_generator(X_memmap, y, batch_size)):\n",
    "            # Here you would do actual training\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Batch {i}/{n_batches_per_epoch}\")\n",
    "            if i >= 20:  # Demo limit\n",
    "                break\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Run pipeline\n",
    "config = memory_efficient_pipeline(path/'X_demo.npy', y, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up demo files\n",
    "import shutil\n",
    "\n",
    "if (path/'X_demo.npy').exists():\n",
    "    os.remove(path/'X_demo.npy')\n",
    "    print(\"Cleaned up demo files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **Use np.memmap for large arrays**: Allows working with datasets larger than RAM\n",
    "2. **Mode 'c' for training**: Copy-on-write allows modifications without changing disk data\n",
    "3. **Batch processing**: Load and process data in chunks\n",
    "4. **Fast slicing**: Memmap slicing is nearly as fast as in-memory arrays\n",
    "\n",
    "### Pattern\n",
    "```python\n",
    "# Save array\n",
    "np.save('data.npy', X)\n",
    "\n",
    "# Load as memmap\n",
    "X_memmap = np.load('data.npy', mmap_mode='c')\n",
    "\n",
    "# Use in batches\n",
    "for batch in batch_generator(X_memmap, y):\n",
    "    # Train on batch\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference\n",
    "print(\"Memory-Efficient Arrays Quick Reference\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# Save array to disk\")\n",
    "print(\"np.save('data.npy', X)\")\n",
    "print(\"\\n# Load as memmap (copy-on-write)\")\n",
    "print(\"X = np.load('data.npy', mmap_mode='c')\")\n",
    "print(\"\\n# Access slices efficiently\")\n",
    "print(\"batch = X[start:end]\")\n",
    "print(\"\\n# Standardize batch\")\n",
    "print(\"batch_std = tsai_rs.ts_standardize(batch.astype(np.float32), by_sample=True)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
