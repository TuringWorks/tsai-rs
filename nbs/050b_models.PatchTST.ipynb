{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models.PatchTST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PatchTST",
    "",
    "> Patch-based Time Series Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on:",
    "",
    "Nie, Y., et al. (2023). **A Time Series is Worth 64 Words: Long-term Forecasting with Transformers**. ICLR 2023.",
    "",
    "Paper: https://arxiv.org/abs/2211.14730",
    "",
    "Key innovation: Divide time series into patches for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture",
    "",
    "PatchTST divides the time series into patches:",
    "1. **Patching**: Split sequence into non-overlapping or overlapping patches",
    "2. **Embedding**: Project patches to model dimension",
    "3. **Transformer**: Process patch embeddings",
    "4. **Head**: Map to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data('ECG200')",
    "X_train = tsai_rs.ts_standardize(X_train)",
    "",
    "# PatchTST configuration",
    "config = tsai_rs.PatchTSTConfig(",
    "    c_in=X_train.shape[1],",
    "    c_out=len(np.unique(y_train)),",
    "    seq_len=X_train.shape[2],",
    "    patch_len=16,      # Length of each patch",
    "    stride=8,          # Stride between patches",
    "    n_heads=4,",
    "    d_model=128,",
    "    d_ff=256,",
    "    n_layers=3,",
    "    dropout=0.1",
    ")",
    "",
    "# Calculate number of patches",
    "n_patches = (config.seq_len - config.patch_len) // config.stride + 1",
    "",
    "print(f\"PatchTST configuration:\")",
    "print(f\"  Sequence length: {config.seq_len}\")",
    "print(f\"  Patch length: {config.patch_len}\")",
    "print(f\"  Stride: {config.stride}\")",
    "print(f\"  Number of patches: {n_patches}\")",
    "print(f\"  Model dim: {config.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Patching",
    "",
    "1. **Reduced sequence length**: Fewer tokens = faster attention",
    "2. **Local semantics**: Each patch captures local patterns",
    "3. **Channel independence**: Can process channels separately",
    "4. **Better for long sequences**: Scales better than standard TST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Long sequence forecasting",
    "# For a 512-length sequence with patch_len=16, stride=8:",
    "# - Number of patches: (512 - 16) / 8 + 1 = 63 patches",
    "# - Much more efficient than 512 individual tokens",
    "",
    "seq_len = 512",
    "patch_len = 16",
    "stride = 8",
    "n_patches = (seq_len - patch_len) // stride + 1",
    "",
    "print(f\"Long sequence example:\")",
    "print(f\"  Original tokens: {seq_len}\")",
    "print(f\"  After patching: {n_patches}\")",
    "print(f\"  Compression: {seq_len / n_patches:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup",
    "learner = tsai_rs.LearnerConfig(",
    "    lr=1e-4,",
    "    epochs=100,",
    "    batch_size=64,",
    "    optimizer='AdamW',",
    "    weight_decay=1e-2",
    ")",
    "",
    "print(\"Ready for training with PatchTST\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}