{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classification with Transformers in tsai-rs\n",
    "\n",
    "This notebook demonstrates using Transformer architectures (TST and PatchTST) for time series classification using **tsai-rs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TST (Time Series Transformer)\n",
    "\n",
    "Based on:\n",
    "* Zerveas, G., et al. (2020). **A Transformer-based Framework for Multivariate Time Series Representation Learning**.\n",
    "* Vaswani, A., et al. (2017). **Attention is all you need**.\n",
    "\n",
    "Transformers excel at capturing long-range dependencies in sequential data through self-attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TST Configuration Options\n",
    "\n",
    "Key hyperparameters for TST:\n",
    "\n",
    "| Parameter | Description | Typical Values | Default |\n",
    "|-----------|-------------|----------------|--------|\n",
    "| `d_model` | Model dimension | 64-512 | 128 |\n",
    "| `n_heads` | Attention heads | 4-16 | 8 |\n",
    "| `n_layers` | Encoder layers | 2-8 | 3 |\n",
    "| `d_ff` | Feed-forward dim | 128-2048 | 256 |\n",
    "| `dropout` | Encoder dropout | 0.0-0.3 | 0.1 |\n",
    "| `fc_dropout` | Classifier dropout | 0.0-0.8 | 0.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multivariate dataset\n",
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "# Get dimensions\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Variables: {n_vars}, Sequence length: {seq_len}, Classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data (recommended for transformers)\n",
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)\n",
    "\n",
    "print(f\"Standardized mean: {X_train_std.mean():.6f}\")\n",
    "print(f\"Standardized std: {X_train_std.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TST Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic TST configuration\n",
    "tst_basic = tsai_rs.TSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "print(f\"Basic TST: {tst_basic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TST with specific hyperparameters\n",
    "tst_custom = tsai_rs.TSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    d_model=128,      # Model dimension\n",
    "    n_heads=8,        # Number of attention heads\n",
    "    n_layers=3,       # Number of encoder layers\n",
    "    d_ff=256,         # Feed-forward dimension\n",
    "    dropout=0.1,      # Encoder dropout\n",
    "    fc_dropout=0.0    # Classifier dropout\n",
    ")\n",
    "print(f\"Custom TST: {tst_custom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TST with high dropout (for overfitting prevention)\n",
    "tst_regularized = tsai_rs.TSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    d_ff=256,\n",
    "    dropout=0.3,      # Higher encoder dropout\n",
    "    fc_dropout=0.8    # High classifier dropout\n",
    ")\n",
    "print(f\"Regularized TST: {tst_regularized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatchTST Configuration\n",
    "\n",
    "PatchTST divides the time series into patches before applying the transformer, similar to Vision Transformer (ViT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic PatchTST configuration\n",
    "patchtst_basic = tsai_rs.PatchTSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    patch_len=8,      # Length of each patch\n",
    "    stride=8          # Stride between patches (non-overlapping)\n",
    ")\n",
    "print(f\"Basic PatchTST: {patchtst_basic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatchTST with overlapping patches\n",
    "patchtst_overlap = tsai_rs.PatchTSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    patch_len=16,\n",
    "    stride=8,         # Overlapping patches\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    d_ff=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "print(f\"PatchTST (overlapping): {patchtst_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large PatchTST configuration\n",
    "patchtst_large = tsai_rs.PatchTSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    patch_len=16,\n",
    "    stride=8,\n",
    "    d_model=256,\n",
    "    n_heads=16,\n",
    "    n_layers=4,\n",
    "    d_ff=512,\n",
    "    dropout=0.2\n",
    ")\n",
    "print(f\"Large PatchTST: {patchtst_large}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Transformer Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TST vs PatchTST\n",
    "configs = {\n",
    "    'TST (basic)': tsai_rs.TSTConfig(\n",
    "        n_vars=n_vars, seq_len=seq_len, n_classes=n_classes\n",
    "    ),\n",
    "    'TST (custom)': tsai_rs.TSTConfig(\n",
    "        n_vars=n_vars, seq_len=seq_len, n_classes=n_classes,\n",
    "        d_model=128, n_heads=8, n_layers=3, dropout=0.1\n",
    "    ),\n",
    "    'TST (regularized)': tsai_rs.TSTConfig(\n",
    "        n_vars=n_vars, seq_len=seq_len, n_classes=n_classes,\n",
    "        d_model=128, n_heads=8, n_layers=3, dropout=0.3, fc_dropout=0.8\n",
    "    ),\n",
    "    'PatchTST (basic)': tsai_rs.PatchTSTConfig(\n",
    "        n_vars=n_vars, seq_len=seq_len, n_classes=n_classes,\n",
    "        patch_len=8, stride=8\n",
    "    ),\n",
    "    'PatchTST (overlap)': tsai_rs.PatchTSTConfig(\n",
    "        n_vars=n_vars, seq_len=seq_len, n_classes=n_classes,\n",
    "        patch_len=16, stride=8, d_model=128, n_heads=8\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"{'Configuration':<25} {'Details'}\")\n",
    "print(\"-\" * 80)\n",
    "for name, config in configs.items():\n",
    "    print(f\"{name:<25} {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner configuration for transformers\n",
    "# Note: Transformers typically need lower learning rates\n",
    "learner_config = tsai_rs.LearnerConfig(\n",
    "    lr=1e-4,          # Lower lr for transformers\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "print(f\"Learner config: {learner_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-cycle scheduler\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "n_samples = X_train.shape[0]\n",
    "steps_per_epoch = (n_samples + batch_size - 1) // batch_size\n",
    "total_steps = n_epochs * steps_per_epoch\n",
    "\n",
    "scheduler = tsai_rs.OneCycleLR.simple(max_lr=1e-4, total_steps=total_steps)\n",
    "\n",
    "print(f\"Training setup:\")\n",
    "print(f\"  Epochs: {n_epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Total steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lower Learning Rate\n",
    "\n",
    "Transformers typically require lower learning rates (1e-4 to 1e-5) compared to CNNs (1e-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical learning rates for different architectures\n",
    "lr_configs = {\n",
    "    'InceptionTimePlus': 1e-3,\n",
    "    'ResNetPlus': 1e-3,\n",
    "    'TST': 1e-4,\n",
    "    'PatchTST': 1e-4,\n",
    "    'RNNPlus': 1e-3,\n",
    "}\n",
    "\n",
    "print(\"Recommended learning rates:\")\n",
    "for arch, lr in lr_configs.items():\n",
    "    print(f\"  {arch}: {lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Standardize Data by Variable\n",
    "\n",
    "For transformers, standardizing each variable independently often works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize by sample (each sample normalized)\n",
    "X_std_sample = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "print(f\"By sample - Sample 0 mean: {X_std_sample[0].mean():.6f}\")\n",
    "\n",
    "# For transformers, consider standardizing by variable\n",
    "# This maintains relative differences across samples\n",
    "X_std_global = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=False)\n",
    "print(f\"By dataset - Global mean: {X_std_global.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use Dropout to Prevent Overfitting\n",
    "\n",
    "Transformers can easily overfit. Increase dropout and fc_dropout if you see this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout configurations for different overfitting scenarios\n",
    "dropout_configs = {\n",
    "    'No overfitting': {'dropout': 0.1, 'fc_dropout': 0.0},\n",
    "    'Mild overfitting': {'dropout': 0.2, 'fc_dropout': 0.3},\n",
    "    'Moderate overfitting': {'dropout': 0.3, 'fc_dropout': 0.5},\n",
    "    'Severe overfitting': {'dropout': 0.3, 'fc_dropout': 0.8},\n",
    "}\n",
    "\n",
    "print(\"Dropout configurations:\")\n",
    "for scenario, config in dropout_configs.items():\n",
    "    print(f\"  {scenario}: dropout={config['dropout']}, fc_dropout={config['fc_dropout']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choose Patch Size Wisely (PatchTST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of patches for different configurations\n",
    "def calc_n_patches(seq_len, patch_len, stride):\n",
    "    return (seq_len - patch_len) // stride + 1\n",
    "\n",
    "patch_configs = [\n",
    "    (8, 8),    # Non-overlapping small patches\n",
    "    (16, 16),  # Non-overlapping large patches\n",
    "    (16, 8),   # Overlapping patches\n",
    "    (8, 4),    # Dense overlapping\n",
    "]\n",
    "\n",
    "print(f\"For seq_len={seq_len}:\")\n",
    "print(f\"{'Patch':<10} {'Stride':<10} {'# Patches':<15} {'Overlap'}\")\n",
    "print(\"-\" * 50)\n",
    "for patch_len, stride in patch_configs:\n",
    "    n_patches = calc_n_patches(seq_len, patch_len, stride)\n",
    "    overlap = 'No' if patch_len == stride else f'Yes ({patch_len - stride} pts)'\n",
    "    print(f\"{patch_len:<10} {stride:<10} {n_patches:<15} {overlap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Different Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['ECG200', 'GunPoint', 'FordA', 'NATOPS', 'Wafer']\n",
    "\n",
    "print(f\"{'Dataset':<15} {'Vars':<6} {'Len':<8} {'TST d_model':<12} {'PatchTST patch':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dsid in datasets:\n",
    "    try:\n",
    "        X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "        n_vars = X_train.shape[1]\n",
    "        seq_len = X_train.shape[2]\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        \n",
    "        # Suggested configurations\n",
    "        d_model = 128 if seq_len > 50 else 64\n",
    "        patch_len = min(16, seq_len // 4) if seq_len >= 16 else seq_len // 2\n",
    "        \n",
    "        print(f\"{dsid:<15} {n_vars:<6} {seq_len:<8} {d_model:<12} {patch_len:<15}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{dsid:<15} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: TST Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete TST setup example\n",
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "# Get dimensions\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Standardize\n",
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)\n",
    "\n",
    "# Create dataset\n",
    "train_ds = tsai_rs.TSDataset(X_train_std, y_train)\n",
    "test_ds = tsai_rs.TSDataset(X_test_std, y_test)\n",
    "\n",
    "# Configure TST\n",
    "tst_config = tsai_rs.TSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    d_ff=256,\n",
    "    dropout=0.3,\n",
    "    fc_dropout=0.8\n",
    ")\n",
    "\n",
    "# Configure training\n",
    "learner_config = tsai_rs.LearnerConfig(\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"TST config: {tst_config}\")\n",
    "print(f\"Learner config: {learner_config}\")\n",
    "print(f\"\\nReady for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example: PatchTST Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete PatchTST setup example\n",
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "# Get dimensions\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "# Configure PatchTST\n",
    "patchtst_config = tsai_rs.PatchTSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    patch_len=16,\n",
    "    stride=8,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    d_ff=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Calculate number of patches\n",
    "n_patches = (seq_len - 16) // 8 + 1\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Number of patches: {n_patches}\")\n",
    "print(f\"PatchTST config: {patchtst_config}\")\n",
    "print(f\"\\nReady for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated Transformer architectures in tsai-rs:\n",
    "\n",
    "### TST (Time Series Transformer)\n",
    "- Pure transformer for time series\n",
    "- Each timestep is a token\n",
    "- Good for capturing long-range dependencies\n",
    "\n",
    "### PatchTST\n",
    "- Patches time series before applying transformer\n",
    "- Reduces sequence length (better for long series)\n",
    "- Inspired by Vision Transformer (ViT)\n",
    "\n",
    "### Key Tips\n",
    "1. Use lower learning rates (1e-4 to 1e-5)\n",
    "2. Standardize data (by sample or by variable)\n",
    "3. Use dropout to prevent overfitting\n",
    "4. For PatchTST, choose patch size based on sequence length\n",
    "\n",
    "### When to Use Transformers\n",
    "- Long sequences with complex patterns\n",
    "- When capturing long-range dependencies is important\n",
    "- Multivariate time series with cross-variable relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference\n",
    "print(\"TST Configuration:\")\n",
    "print(\"  tsai_rs.TSTConfig(n_vars, seq_len, n_classes,\")\n",
    "print(\"      d_model=128, n_heads=8, n_layers=3,\")\n",
    "print(\"      d_ff=256, dropout=0.3, fc_dropout=0.8)\")\n",
    "\n",
    "print(\"\\nPatchTST Configuration:\")\n",
    "print(\"  tsai_rs.PatchTSTConfig(n_vars, seq_len, n_classes,\")\n",
    "print(\"      patch_len=16, stride=8,\")\n",
    "print(\"      d_model=128, n_heads=8, n_layers=3)\")\n",
    "\n",
    "print(\"\\nRecommended Learning Rate: 1e-4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
