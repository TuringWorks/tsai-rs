{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Dynamics and Analysis with tsai-rs\n",
    "\n",
    "This notebook demonstrates how to analyze model predictions and understand prediction dynamics using **tsai-rs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "Understanding model predictions is crucial for:\n",
    "1. **Debugging**: Identify why certain samples are misclassified\n",
    "2. **Model improvement**: Focus on hard examples\n",
    "3. **Trust**: Build confidence in model predictions\n",
    "4. **Data quality**: Find mislabeled samples\n",
    "\n",
    "Based on advice from Andrej Karpathy's blog post on neural network training:\n",
    "> \"Visualize prediction dynamics: I like to visualize model predictions on a fixed test batch during the course of training. The 'dynamics' of how these predictions move will give you incredibly good intuition for how the training progresses.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Classes: {np.unique(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Model Predictions\n",
    "\n",
    "For demonstration, we'll simulate model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_predictions(y_true, n_classes, accuracy=0.85, seed=42):\n",
    "    \"\"\"Simulate model predictions with specified accuracy.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    # Create predicted probabilities\n",
    "    probs = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for i, true_label in enumerate(y_true):\n",
    "        # With probability 'accuracy', predict correct class\n",
    "        if np.random.random() < accuracy:\n",
    "            # Correct prediction - high confidence\n",
    "            probs[i, int(true_label)] = np.random.uniform(0.6, 0.95)\n",
    "            # Distribute remaining probability\n",
    "            remaining = 1 - probs[i, int(true_label)]\n",
    "            for j in range(n_classes):\n",
    "                if j != int(true_label):\n",
    "                    probs[i, j] = remaining * np.random.random()\n",
    "        else:\n",
    "            # Incorrect prediction\n",
    "            wrong_class = np.random.choice([c for c in range(n_classes) if c != int(true_label)])\n",
    "            probs[i, wrong_class] = np.random.uniform(0.4, 0.8)\n",
    "            remaining = 1 - probs[i, wrong_class]\n",
    "            for j in range(n_classes):\n",
    "                if j != wrong_class:\n",
    "                    probs[i, j] = remaining * np.random.random()\n",
    "        \n",
    "        # Normalize\n",
    "        probs[i] = probs[i] / probs[i].sum()\n",
    "    \n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    return y_pred, probs\n",
    "\n",
    "# Simulate predictions\n",
    "y_pred, probs = simulate_predictions(y_test, n_classes, accuracy=0.80)\n",
    "\n",
    "actual_acc = (y_pred == y_test).mean()\n",
    "print(f\"Simulated accuracy: {actual_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix using tsai-rs\n",
    "cm = tsai_rs.confusion_matrix(y_test, y_pred, n_classes)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f'Confusion Matrix - Accuracy: {actual_acc:.2%}')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_classes):\n",
    "    for j in range(n_classes):\n",
    "        text = ax.text(j, i, int(cm[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Losses Analysis\n",
    "\n",
    "Identifying samples with the highest losses helps understand where the model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute losses (negative log probability of true class)\n",
    "losses = -np.log(probs[np.arange(len(y_test)), y_test.astype(int)] + 1e-10)\n",
    "\n",
    "# Get top losses using tsai-rs\n",
    "top_k = 10\n",
    "top_loss_indices = tsai_rs.top_losses(losses, k=top_k)\n",
    "\n",
    "print(f\"Top {top_k} losses:\")\n",
    "print(f\"{'Index':<8} {'True':<8} {'Pred':<8} {'Loss':<10} {'Confidence':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for idx in top_loss_indices:\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred[idx]\n",
    "    loss = losses[idx]\n",
    "    conf = probs[idx].max()\n",
    "    marker = \"WRONG\" if true_label != pred_label else \"\"\n",
    "    print(f\"{idx:<8} {true_label:<8} {pred_label:<8} {loss:<10.4f} {conf:<12.4f} {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Top Loss Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time series with highest losses\n",
    "n_show = min(6, len(top_loss_indices))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(top_loss_indices[:n_show]):\n",
    "    ts = X_test_std[idx, 0, :]  # First variable\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred[idx]\n",
    "    conf = probs[idx].max()\n",
    "    \n",
    "    axes[i].plot(ts)\n",
    "    color = 'red' if true_label != pred_label else 'green'\n",
    "    axes[i].set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {conf:.2%}', color=color)\n",
    "\n",
    "plt.suptitle('Top Loss Samples (Red = Misclassified)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence distribution\n",
    "confidences = probs.max(axis=1)\n",
    "correct_mask = y_pred == y_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "axes[0].hist(confidences, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(confidences.mean(), color='red', linestyle='--', label=f'Mean: {confidences.mean():.2f}')\n",
    "axes[0].set_xlabel('Confidence')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Overall Confidence Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Confidence: correct vs incorrect\n",
    "axes[1].hist(confidences[correct_mask], bins=15, alpha=0.6, label=f'Correct (n={correct_mask.sum()})', color='green')\n",
    "axes[1].hist(confidences[~correct_mask], bins=15, alpha=0.6, label=f'Incorrect (n={(~correct_mask).sum()})', color='red')\n",
    "axes[1].set_xlabel('Confidence')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Confidence: Correct vs Incorrect Predictions')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean confidence (correct): {confidences[correct_mask].mean():.3f}\")\n",
    "print(f\"Mean confidence (incorrect): {confidences[~correct_mask].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-class metrics\n",
    "classes = np.unique(y_test)\n",
    "\n",
    "print(f\"{'Class':<10} {'Samples':<10} {'Accuracy':<12} {'Avg Conf':<12} {'Avg Loss'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "class_stats = []\n",
    "for cls in classes:\n",
    "    mask = y_test == cls\n",
    "    n_samples = mask.sum()\n",
    "    acc = (y_pred[mask] == y_test[mask]).mean()\n",
    "    avg_conf = confidences[mask].mean()\n",
    "    avg_loss = losses[mask].mean()\n",
    "    \n",
    "    class_stats.append((cls, n_samples, acc, avg_conf, avg_loss))\n",
    "    print(f\"{cls:<10} {n_samples:<10} {acc:<12.2%} {avg_conf:<12.3f} {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class accuracy\n",
    "class_labels = [str(s[0]) for s in class_stats]\n",
    "accuracies = [s[2] for s in class_stats]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(class_labels, accuracies, color='steelblue', edgecolor='black')\n",
    "ax.axhline(actual_acc, color='red', linestyle='--', label=f'Overall: {actual_acc:.2%}')\n",
    "\n",
    "# Color bars based on performance\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    if acc < actual_acc - 0.1:\n",
    "        bar.set_color('salmon')\n",
    "    elif acc > actual_acc + 0.1:\n",
    "        bar.set_color('lightgreen')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Per-Class Accuracy')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common misclassification pairs\n",
    "misclassified_mask = y_pred != y_test\n",
    "misclassified_pairs = list(zip(y_test[misclassified_mask], y_pred[misclassified_mask]))\n",
    "\n",
    "from collections import Counter\n",
    "pair_counts = Counter(misclassified_pairs)\n",
    "\n",
    "print(\"Most common misclassifications:\")\n",
    "print(f\"{'True':<10} {'Predicted':<12} {'Count':<10} {'% of Errors'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "total_errors = len(misclassified_pairs)\n",
    "for (true_cls, pred_cls), count in pair_counts.most_common(10):\n",
    "    pct = count / total_errors * 100\n",
    "    print(f\"{true_cls:<10} {pred_cls:<12} {count:<10} {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard vs Easy Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize samples by difficulty\n",
    "easy_threshold = 0.9\n",
    "hard_threshold = 0.6\n",
    "\n",
    "easy_mask = (confidences >= easy_threshold) & correct_mask\n",
    "hard_mask = (confidences < hard_threshold) | ~correct_mask\n",
    "medium_mask = ~easy_mask & ~hard_mask\n",
    "\n",
    "print(\"Sample Difficulty Distribution:\")\n",
    "print(f\"  Easy (conf >= {easy_threshold}, correct):   {easy_mask.sum()} ({easy_mask.mean():.1%})\")\n",
    "print(f\"  Medium:                                    {medium_mask.sum()} ({medium_mask.mean():.1%})\")\n",
    "print(f\"  Hard (conf < {hard_threshold} or wrong):   {hard_mask.sum()} ({hard_mask.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare easy vs hard samples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Easy samples\n",
    "easy_indices = np.where(easy_mask)[0][:3]\n",
    "for i, idx in enumerate(easy_indices):\n",
    "    ts = X_test_std[idx, 0, :]\n",
    "    axes[0, i].plot(ts, color='green')\n",
    "    axes[0, i].set_title(f'Easy - Conf: {confidences[idx]:.2%}')\n",
    "axes[0, 0].set_ylabel('Easy Samples')\n",
    "\n",
    "# Hard samples\n",
    "hard_indices = np.where(hard_mask)[0][:3]\n",
    "for i, idx in enumerate(hard_indices):\n",
    "    ts = X_test_std[idx, 0, :]\n",
    "    color = 'red' if y_pred[idx] != y_test[idx] else 'orange'\n",
    "    axes[1, i].plot(ts, color=color)\n",
    "    status = 'Wrong' if y_pred[idx] != y_test[idx] else 'Low conf'\n",
    "    axes[1, i].set_title(f'Hard ({status}) - Conf: {confidences[idx]:.2%}')\n",
    "axes[1, 0].set_ylabel('Hard Samples')\n",
    "\n",
    "plt.suptitle('Comparison: Easy vs Hard Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights from Prediction Dynamics\n",
    "\n",
    "When visualizing predictions during training, look for:\n",
    "\n",
    "1. **Easy vs difficult classes**: Which classes converge first?\n",
    "2. **Stability**: Do predictions oscillate or converge smoothly?\n",
    "3. **Class imbalance effects**: Are frequent classes dominating?\n",
    "4. **Learning rate effects**: Too high LR causes instability\n",
    "5. **Overfitting signals**: Training predictions perfect, validation unstable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated prediction analysis techniques:\n",
    "\n",
    "### Analysis Tools\n",
    "- `confusion_matrix`: Visualize classification performance\n",
    "- `top_losses`: Identify hardest samples\n",
    "\n",
    "### Key Insights\n",
    "1. **Confidence analysis**: Model confidence correlates with correctness\n",
    "2. **Per-class performance**: Some classes may be harder\n",
    "3. **Common errors**: Identify frequently confused class pairs\n",
    "4. **Hard samples**: Focus improvement efforts on difficult cases\n",
    "\n",
    "### Applications\n",
    "- Model debugging and improvement\n",
    "- Data quality assessment\n",
    "- Building trust in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference\n",
    "print(\"Prediction Analysis Functions:\")\n",
    "print(\"==============================\")\n",
    "print(\"\\n# Confusion matrix\")\n",
    "print(\"cm = tsai_rs.confusion_matrix(y_true, y_pred, n_classes)\")\n",
    "print(\"\\n# Top losses\")\n",
    "print(\"indices = tsai_rs.top_losses(losses, k=10)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
