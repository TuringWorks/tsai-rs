{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Train with Big Arrays Faster using tsai-rs\n",
    "\n",
    "This notebook demonstrates efficient training strategies for larger-than-memory datasets using **tsai-rs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "When training models on large datasets, the bottleneck is often data loading rather than GPU computation.\n",
    "\n",
    "Key concepts covered:\n",
    "1. **Memory-mapped arrays**: Keep data on disk, load only what's needed\n",
    "2. **Efficient batching**: Create batches without loading full dataset\n",
    "3. **Parallel data loading**: Use multiple workers for faster loading\n",
    "4. **Data chunking**: Optimize chunk sizes for your workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Mapped Arrays\n",
    "\n",
    "Memory-mapped arrays (`np.memmap`) allow you to work with large datasets that don't fit in memory.\n",
    "\n",
    "Key benefits:\n",
    "- Data stays on disk until accessed\n",
    "- Only the accessed portion is loaded into memory\n",
    "- Efficient for creating batches from large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data directory\n",
    "path = Path('data')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Create a moderate-sized array for demonstration\n",
    "n_samples = 10000\n",
    "n_vars = 10\n",
    "seq_len = 100\n",
    "\n",
    "arr = np.random.rand(n_samples, n_vars, seq_len).astype(np.float32)\n",
    "np.save(path/'arr.npy', arr)\n",
    "\n",
    "print(f\"Array shape: {arr.shape}\")\n",
    "print(f\"Array size: {arr.nbytes / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load as Memory-Mapped Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as memory-mapped array\n",
    "# mmap_mode options:\n",
    "# 'r'  - read only\n",
    "# 'r+' - read and write\n",
    "# 'c'  - copy-on-write (changes in memory only)\n",
    "\n",
    "memmap_arr = np.load(path/'arr.npy', mmap_mode='c')\n",
    "print(f\"Type: {type(memmap_arr)}\")\n",
    "print(f\"Shape: {memmap_arr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance: In-Memory vs Memory-Mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch creation\n",
    "batch_size = 64\n",
    "\n",
    "def create_batch_indices(n_samples, batch_size):\n",
    "    return np.random.choice(n_samples, batch_size, replace=False)\n",
    "\n",
    "# In-memory array indexing\n",
    "n_iterations = 100\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    idx = create_batch_indices(n_samples, batch_size)\n",
    "    batch = arr[idx]\n",
    "elapsed_memory = time.time() - start\n",
    "\n",
    "# Memory-mapped array indexing\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    idx = create_batch_indices(n_samples, batch_size)\n",
    "    batch = memmap_arr[idx]\n",
    "elapsed_memmap = time.time() - start\n",
    "\n",
    "print(f\"In-memory: {elapsed_memory:.4f}s for {n_iterations} batches\")\n",
    "print(f\"Memory-mapped: {elapsed_memmap:.4f}s for {n_iterations} batches\")\n",
    "print(f\"Ratio: {elapsed_memmap/elapsed_memory:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Data Loading with tsai-rs\n",
    "\n",
    "tsai-rs provides efficient data structures for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSDataset for Efficient Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real dataset\n",
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"Shape: {X_train.shape}\")\n",
    "print(f\"Variables: {n_vars}, Sequence length: {seq_len}, Classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)\n",
    "\n",
    "# Create TSDataset\n",
    "train_ds = tsai_rs.TSDataset(X_train_std, y_train)\n",
    "test_ds = tsai_rs.TSDataset(X_test_std, y_test)\n",
    "\n",
    "print(f\"Train dataset: {train_ds}\")\n",
    "print(f\"Test dataset: {test_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark batch creation with numpy\n",
    "batch_size = 64\n",
    "n_iterations = 1000\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    idx = np.random.choice(len(X_train_std), batch_size, replace=False)\n",
    "    X_batch = X_train_std[idx]\n",
    "    y_batch = y_train[idx]\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Numpy batch creation: {elapsed*1000/n_iterations:.3f}ms per batch\")\n",
    "print(f\"Batches per second: {n_iterations/elapsed:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for Large Datasets\n",
    "\n",
    "When working with larger-than-memory datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use Memory-Mapped Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to disk\n",
    "np.save(path/'X_processed.npy', X_train_std)\n",
    "np.save(path/'y_processed.npy', y_train)\n",
    "\n",
    "# Load as memory-mapped\n",
    "X_mmap = np.load(path/'X_processed.npy', mmap_mode='c')\n",
    "y_mmap = np.load(path/'y_processed.npy', mmap_mode='c')\n",
    "\n",
    "print(f\"X_mmap type: {type(X_mmap)}\")\n",
    "print(f\"y_mmap type: {type(y_mmap)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Process Data in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_chunks(X, chunksize=1000):\n",
    "    \"\"\"Process large arrays in chunks to avoid memory issues.\"\"\"\n",
    "    n_samples = len(X)\n",
    "    n_chunks = (n_samples + chunksize - 1) // chunksize\n",
    "    \n",
    "    results = []\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunksize\n",
    "        end = min((i + 1) * chunksize, n_samples)\n",
    "        \n",
    "        # Process chunk\n",
    "        chunk = X[start:end]\n",
    "        processed = tsai_rs.ts_standardize(chunk.astype(np.float32), by_sample=True)\n",
    "        results.append(processed)\n",
    "    \n",
    "    return np.concatenate(results, axis=0)\n",
    "\n",
    "# Example usage (with small data for demonstration)\n",
    "X_chunked = process_in_chunks(X_train, chunksize=50)\n",
    "print(f\"Processed shape: {X_chunked.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Efficient Index Sorting\n",
    "\n",
    "Sorting indices before accessing memory-mapped arrays can improve performance due to sequential disk access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sorted vs unsorted index access\n",
    "n_iterations = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Unsorted indices\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    idx = np.random.choice(len(X_mmap), batch_size, replace=False)\n",
    "    batch = X_mmap[idx]\n",
    "elapsed_unsorted = time.time() - start\n",
    "\n",
    "# Sorted indices\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    idx = np.sort(np.random.choice(len(X_mmap), batch_size, replace=False))\n",
    "    batch = X_mmap[idx]\n",
    "elapsed_sorted = time.time() - start\n",
    "\n",
    "print(f\"Unsorted indices: {elapsed_unsorted:.4f}s\")\n",
    "print(f\"Sorted indices: {elapsed_sorted:.4f}s\")\n",
    "print(f\"Improvement: {(elapsed_unsorted - elapsed_sorted) / elapsed_unsorted * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration for Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "# Training configuration optimized for large datasets\n",
    "learner_config = tsai_rs.LearnerConfig(\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "\n",
    "print(f\"Model config: {config}\")\n",
    "print(f\"Learner config: {learner_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Batch Sizes\n",
    "\n",
    "Larger batch sizes can improve GPU utilization but require more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory estimation for different batch sizes\n",
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "\n",
    "print(f\"{'Batch Size':<12} {'Memory (MB)':<15} {'Batches/Epoch'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    # Estimate memory for batch\n",
    "    batch_memory = bs * n_vars * seq_len * 4 / 1e6  # float32 = 4 bytes\n",
    "    n_batches = (len(X_train) + bs - 1) // bs\n",
    "    print(f\"{bs:<12} {batch_memory:<15.2f} {n_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Learnings\n",
    "\n",
    "### For In-Memory Datasets\n",
    "- Use numpy arrays directly\n",
    "- tsai-rs `TSDataset` provides efficient batching\n",
    "- Standardize data once before training\n",
    "\n",
    "### For Larger-than-Memory Datasets\n",
    "1. **Use memory-mapped arrays** (`np.load(..., mmap_mode='c')`)\n",
    "2. **Sort indices** before accessing disk-based arrays\n",
    "3. **Process in chunks** to avoid memory issues\n",
    "4. **Use appropriate batch sizes** for your GPU memory\n",
    "\n",
    "### Performance Tips\n",
    "- Profile your data loading to identify bottlenecks\n",
    "- Keep frequently accessed data in memory\n",
    "- Use SSD storage for memory-mapped arrays\n",
    "- Consider data augmentation at batch time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import os\n",
    "\n",
    "for fname in ['arr.npy', 'X_processed.npy', 'y_processed.npy']:\n",
    "    fpath = path / fname\n",
    "    if fpath.exists():\n",
    "        os.remove(fpath)\n",
    "        print(f\"Removed {fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered strategies for efficient training with large datasets:\n",
    "\n",
    "### Key Techniques\n",
    "- Memory-mapped arrays for disk-based data\n",
    "- Chunk-based processing\n",
    "- Index sorting for sequential access\n",
    "- Batch size optimization\n",
    "\n",
    "### tsai-rs Tools\n",
    "```python\n",
    "# Data loading and standardization\n",
    "X_std = tsai_rs.ts_standardize(X.astype(np.float32), by_sample=True)\n",
    "\n",
    "# Efficient dataset\n",
    "ds = tsai_rs.TSDataset(X_std, y)\n",
    "\n",
    "# Model configuration\n",
    "config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference\n",
    "print(\"Large Dataset Training Quick Reference\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# Memory-mapped loading\")\n",
    "print(\"X_mmap = np.load('data.npy', mmap_mode='c')\")\n",
    "print(\"\\n# Sorted index access (faster for disk)\")\n",
    "print(\"idx = np.sort(np.random.choice(n, batch_size, replace=False))\")\n",
    "print(\"batch = X_mmap[idx]\")\n",
    "print(\"\\n# Standardization\")\n",
    "print(\"X_std = tsai_rs.ts_standardize(X.astype(np.float32), by_sample=True)\")\n",
    "print(\"\\n# TSDataset\")\n",
    "print(\"ds = tsai_rs.TSDataset(X_std, y)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
