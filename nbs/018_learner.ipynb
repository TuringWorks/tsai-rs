{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner",
    "",
    "> Training configuration and management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides:",
    "- LearnerConfig for training parameters",
    "- OneCycleLR scheduler",
    "- Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LearnerConfig",
    "",
    "Configuration for training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic learner configuration",
    "learner_config = tsai_rs.LearnerConfig(",
    "    lr=1e-3,            # Learning rate",
    "    epochs=100,         # Number of epochs",
    "    batch_size=64,      # Batch size",
    "    optimizer='AdamW',  # Optimizer type",
    "    weight_decay=1e-2   # Weight decay for regularization",
    ")",
    "",
    "print(f\"LearnerConfig:\")",
    "print(f\"  Learning rate: {learner_config.lr}\")",
    "print(f\"  Epochs: {learner_config.epochs}\")",
    "print(f\"  Batch size: {learner_config.batch_size}\")",
    "print(f\"  Optimizer: {learner_config.optimizer}\")",
    "print(f\"  Weight decay: {learner_config.weight_decay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneCycleLR Scheduler",
    "",
    "The 1cycle learning rate policy for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneCycle learning rate scheduler",
    "scheduler = tsai_rs.OneCycleLR(",
    "    max_lr=1e-3,           # Maximum learning rate",
    "    epochs=100,            # Number of epochs",
    "    pct_start=0.3,         # Percentage of cycle for warmup",
    "    div_factor=25.0,       # Initial lr = max_lr / div_factor",
    "    final_div_factor=10000.0  # Final lr = max_lr / final_div_factor",
    ")",
    "",
    "print(f\"OneCycleLR:\")",
    "print(f\"  Max LR: {scheduler.max_lr}\")",
    "print(f\"  Warmup: {scheduler.pct_start * 100}%\")",
    "print(f\"  Initial LR: {scheduler.max_lr / scheduler.div_factor}\")",
    "print(f\"  Final LR: {scheduler.max_lr / scheduler.final_div_factor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Ranges",
    "",
    "Recommended learning rates for different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical learning rate ranges for different models",
    "lr_ranges = {",
    "    'InceptionTimePlus': (1e-4, 3e-3),",
    "    'ResNetPlus': (1e-4, 1e-2),",
    "    'TSTPlus': (1e-5, 1e-3),",
    "    'PatchTST': (1e-5, 1e-3),",
    "    'RNNPlus': (1e-4, 1e-2),",
    "    'MiniRocket': (1e-3, 1e-1)  # Linear classifier",
    "}",
    "",
    "print(\"Recommended learning rate ranges:\")",
    "for model, (min_lr, max_lr) in lr_ranges.items():",
    "    print(f\"  {model}: {min_lr} - {max_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training setup example",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data('ECG200')",
    "X_train = tsai_rs.ts_standardize(X_train)",
    "X_test = tsai_rs.ts_standardize(X_test)",
    "",
    "# Model configuration",
    "model_config = tsai_rs.InceptionTimePlusConfig(",
    "    c_in=X_train.shape[1],",
    "    c_out=len(np.unique(y_train)),",
    "    seq_len=X_train.shape[2],",
    "    nf=32",
    ")",
    "",
    "# Training configuration",
    "learner = tsai_rs.LearnerConfig(",
    "    lr=3e-4,",
    "    epochs=100,",
    "    batch_size=64,",
    "    optimizer='AdamW',",
    "    weight_decay=1e-2",
    ")",
    "",
    "# Learning rate scheduler",
    "scheduler = tsai_rs.OneCycleLR(",
    "    max_lr=3e-4,",
    "    epochs=100,",
    "    pct_start=0.3",
    ")",
    "",
    "print(\"Training setup complete:\")",
    "print(f\"  Model: InceptionTimePlus\")",
    "print(f\"  Train samples: {len(X_train)}\")",
    "print(f\"  Test samples: {len(X_test)}\")",
    "print(f\"  Epochs: {learner.epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Options",
    "",
    "Supported optimizers:",
    "- **SGD**: Basic stochastic gradient descent",
    "- **Adam**: Adaptive moment estimation",
    "- **AdamW**: Adam with decoupled weight decay",
    "- **RAdam**: Rectified Adam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}