{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis",
    "",
    "> Tools for analyzing model predictions and errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides:",
    "- Confusion matrix analysis",
    "- Top losses identification",
    "- Feature importance",
    "- Prediction visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample predictions",
    "np.random.seed(42)",
    "y_true = np.random.randint(0, 3, 100)",
    "y_pred = y_true.copy()",
    "# Add some errors",
    "error_idx = np.random.choice(100, 15, replace=False)",
    "y_pred[error_idx] = np.random.randint(0, 3, 15)",
    "",
    "# Compute confusion matrix",
    "cm = tsai_rs.confusion_matrix(y_true, y_pred)",
    "print(f\"Confusion Matrix (3-class):\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confusion_matrix(cm, class_names=None):",
    "    \"\"\"Analyze a confusion matrix.\"\"\"",
    "    n_classes = len(cm)",
    "    if class_names is None:",
    "        class_names = [f\"Class {i}\" for i in range(n_classes)]",
    "    ",
    "    total = cm.sum()",
    "    correct = np.trace(cm)",
    "    accuracy = correct / total",
    "    ",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")",
    "    print(f\"Total samples: {total}\")",
    "    print(f\"Correct: {correct}\")",
    "    print()",
    "    ",
    "    for i in range(n_classes):",
    "        tp = cm[i, i]",
    "        fp = cm[:, i].sum() - tp",
    "        fn = cm[i, :].sum() - tp",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0",
    "        ",
    "        print(f\"{class_names[i]}:\")",
    "        print(f\"  Precision: {precision:.4f}\")",
    "        print(f\"  Recall: {recall:.4f}\")",
    "        print(f\"  F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the confusion matrix",
    "analyze_confusion_matrix(cm, ['Class A', 'Class B', 'Class C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Losses Analysis",
    "",
    "Identify samples with the highest loss for error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top losses using tsai_rs",
    "losses = np.random.exponential(0.3, 100)  # Simulated loss values",
    "top_idx = tsai_rs.top_losses(losses, k=5)",
    "",
    "print(f\"Top 5 loss indices: {top_idx}\")",
    "print(f\"Top 5 loss values: {losses[top_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_losses(losses, y_true, y_pred, k=10):",
    "    \"\"\"Analyze samples with highest loss.\"\"\"",
    "    top_idx = tsai_rs.top_losses(losses, k=k)",
    "    ",
    "    print(f\"Top {k} losses:\")",
    "    print(\"-\" * 40)",
    "    for i, idx in enumerate(top_idx):",
    "        print(f\"{i+1}. Index {idx}: Loss={losses[idx]:.4f}, True={y_true[idx]}, Pred={y_pred[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top losses",
    "analyze_top_losses(losses, y_true, y_pred, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(y_true, y_pred):",
    "    \"\"\"Analyze prediction errors.\"\"\"",
    "    errors = y_true != y_pred",
    "    n_errors = errors.sum()",
    "    n_total = len(y_true)",
    "    ",
    "    print(f\"Error Analysis:\")",
    "    print(f\"  Total samples: {n_total}\")",
    "    print(f\"  Errors: {n_errors}\")",
    "    print(f\"  Error rate: {n_errors/n_total:.4f}\")",
    "    print()",
    "    ",
    "    # Error breakdown by true class",
    "    classes = np.unique(y_true)",
    "    print(\"Errors by true class:\")",
    "    for c in classes:",
    "        mask = y_true == c",
    "        class_errors = errors[mask].sum()",
    "        class_total = mask.sum()",
    "        print(f\"  Class {c}: {class_errors}/{class_total} ({class_errors/class_total:.4f})\")",
    "    ",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run error analysis",
    "errors = error_analysis(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}