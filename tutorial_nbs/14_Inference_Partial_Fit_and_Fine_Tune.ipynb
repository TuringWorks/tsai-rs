{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference, Partial Fit, and Fine-Tuning with tsai-rs\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. **Save and load models** for later inference\n",
    "2. **Continue training** (partial fit) on new data\n",
    "3. **Fine-tune** pretrained models on new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "In real-world applications, you often need to:\n",
    "- Deploy models and generate predictions (inference)\n",
    "- Update models with new data (incremental learning)\n",
    "- Adapt pretrained models to new tasks (transfer learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "model_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "\n",
    "# Create training configuration\n",
    "learner_config = tsai_rs.LearnerConfig(\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = tsai_rs.TSDataset(X_train_std, y_train)\n",
    "test_ds = tsai_rs.TSDataset(X_test_std, y_test)\n",
    "\n",
    "print(f\"Model config: {model_config}\")\n",
    "print(f\"Learner config: {learner_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real scenario, you would train the model here\n",
    "# For demonstration, we'll simulate training metrics\n",
    "\n",
    "def simulate_training_history(n_epochs=10):\n",
    "    \"\"\"Simulate training history.\"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 1.0 * np.exp(-epoch/3) + np.random.normal(0, 0.02)\n",
    "        val_loss = 1.1 * np.exp(-epoch/3) + np.random.normal(0, 0.03)\n",
    "        val_acc = 0.5 + 0.4 * (1 - np.exp(-epoch/3)) + np.random.normal(0, 0.01)\n",
    "        \n",
    "        history['train_loss'].append(max(0, train_loss))\n",
    "        history['val_loss'].append(max(0, val_loss))\n",
    "        history['val_accuracy'].append(np.clip(val_acc, 0, 1))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"Training model...\")\n",
    "print(\"-\" * 60)\n",
    "history = simulate_training_history(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create models directory\n",
    "models_path = Path('models')\n",
    "models_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model configuration\n",
    "model_info = {\n",
    "    'architecture': 'InceptionTimePlus',\n",
    "    'n_vars': n_vars,\n",
    "    'seq_len': seq_len,\n",
    "    'n_classes': n_classes,\n",
    "    'dataset': dsid,\n",
    "    'training': {\n",
    "        'lr': 1e-3,\n",
    "        'weight_decay': 0.01,\n",
    "        'n_epochs': 10,\n",
    "        'final_accuracy': history['val_accuracy'][-1]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(models_path / 'model_config.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model configuration saved to models/model_config.json\")\n",
    "print(json.dumps(model_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Inference (Generating Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model configuration\n",
    "with open(models_path / 'model_config.json', 'r') as f:\n",
    "    loaded_config = json.load(f)\n",
    "\n",
    "print(\"Loaded model configuration:\")\n",
    "print(json.dumps(loaded_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model configuration from saved info\n",
    "inference_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=loaded_config['n_vars'],\n",
    "    seq_len=loaded_config['seq_len'],\n",
    "    n_classes=loaded_config['n_classes']\n",
    ")\n",
    "\n",
    "print(f\"Inference config: {inference_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_predictions(X, n_classes):\n",
    "    \"\"\"Simulate model predictions.\"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Generate random probabilities\n",
    "    probs = np.random.rand(n_samples, n_classes)\n",
    "    probs = probs / probs.sum(axis=1, keepdims=True)  # Normalize\n",
    "    \n",
    "    # Get predicted classes\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "    \n",
    "    return probs, preds\n",
    "\n",
    "# Generate predictions on test data\n",
    "probs, preds = simulate_predictions(X_test_std, n_classes)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Probabilities shape: {probs.shape}\")\n",
    "print(f\"\\nFirst 10 predictions: {preds[:10]}\")\n",
    "print(f\"First 10 true labels: {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample inference\n",
    "single_sample = X_test_std[0:1]  # Shape: (1, n_vars, seq_len)\n",
    "single_probs, single_pred = simulate_predictions(single_sample, n_classes)\n",
    "\n",
    "print(f\"Single sample shape: {single_sample.shape}\")\n",
    "print(f\"Prediction: {single_pred[0]}\")\n",
    "print(f\"Probabilities: {single_probs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Partial Fit (Incremental Learning)\n",
    "\n",
    "Continue training with new data while preserving learned knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate new incoming data\n",
    "# In practice, this would be newly collected samples\n",
    "n_new_samples = 50\n",
    "new_X = X_train_std[:n_new_samples].copy()  # Simulate new data\n",
    "new_y = y_train[:n_new_samples].copy()\n",
    "\n",
    "# Add some augmentation to simulate different data\n",
    "new_X = tsai_rs.add_gaussian_noise(new_X, std=0.1, seed=42)\n",
    "\n",
    "print(f\"New data shape: {new_X.shape}\")\n",
    "print(f\"New labels: {np.unique(new_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for partial fit (typically lower learning rate)\n",
    "partial_fit_config = tsai_rs.LearnerConfig(\n",
    "    lr=1e-4,  # Lower LR for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "\n",
    "print(f\"Partial fit config: {partial_fit_config}\")\n",
    "print(\"\\nKey considerations for partial fit:\")\n",
    "print(\"  1. Use lower learning rate (e.g., 1e-4 vs 1e-3)\")\n",
    "print(\"  2. Train for fewer epochs\")\n",
    "print(\"  3. Monitor for catastrophic forgetting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate partial fit training\n",
    "print(\"\\nPartial fit training on new data...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(3):  # Fewer epochs for partial fit\n",
    "    # Simulate metrics that improve slowly (model already trained)\n",
    "    train_loss = 0.3 + 0.1 * np.exp(-epoch) + np.random.normal(0, 0.01)\n",
    "    val_acc = 0.85 + 0.05 * (1 - np.exp(-epoch)) + np.random.normal(0, 0.01)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fine-Tuning (Transfer Learning)\n",
    "\n",
    "Adapt a model trained on one dataset to a different but related dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a different dataset for fine-tuning\n",
    "target_dsid = 'ECG200'\n",
    "X_target_train, y_target_train, X_target_test, y_target_test = tsai_rs.get_UCR_data(\n",
    "    target_dsid, return_split=True\n",
    ")\n",
    "\n",
    "target_n_vars = X_target_train.shape[1]\n",
    "target_seq_len = X_target_train.shape[2]\n",
    "target_n_classes = len(np.unique(y_target_train))\n",
    "\n",
    "print(f\"Target dataset: {target_dsid}\")\n",
    "print(f\"Shape: {X_target_train.shape}\")\n",
    "print(f\"Classes: {target_n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize target data\n",
    "X_target_train_std = tsai_rs.ts_standardize(X_target_train.astype(np.float32), by_sample=True)\n",
    "X_target_test_std = tsai_rs.ts_standardize(X_target_test.astype(np.float32), by_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for target dataset\n",
    "# Note: Architecture may need adjustment for different input dimensions\n",
    "target_model_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=target_n_vars,\n",
    "    seq_len=target_seq_len,\n",
    "    n_classes=target_n_classes\n",
    ")\n",
    "\n",
    "print(f\"Target model config: {target_model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning typically uses:\n",
    "# 1. Lower learning rate\n",
    "# 2. Gradual unfreezing (train head first, then full model)\n",
    "# 3. Discriminative learning rates (different LR for different layers)\n",
    "\n",
    "fine_tune_strategies = {\n",
    "    'Phase 1 - Train head only': {\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 3,\n",
    "        'freeze_backbone': True\n",
    "    },\n",
    "    'Phase 2 - Train full model': {\n",
    "        'lr': 1e-4,\n",
    "        'epochs': 5,\n",
    "        'freeze_backbone': False\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Fine-tuning strategy:\")\n",
    "for phase, params in fine_tune_strategies.items():\n",
    "    print(f\"\\n{phase}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fine-tuning\n",
    "print(\"\\nFine-tuning simulation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Phase 1: Train head only\n",
    "print(\"\\nPhase 1: Training classification head...\")\n",
    "print(\"-\" * 40)\n",
    "for epoch in range(3):\n",
    "    train_loss = 0.8 * np.exp(-epoch) + np.random.normal(0, 0.02)\n",
    "    val_acc = 0.6 + 0.2 * (1 - np.exp(-epoch)) + np.random.normal(0, 0.02)\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "# Phase 2: Train full model\n",
    "print(\"\\nPhase 2: Training full model...\")\n",
    "print(\"-\" * 40)\n",
    "base_acc = 0.8\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.4 * np.exp(-epoch/2) + np.random.normal(0, 0.01)\n",
    "    val_acc = base_acc + 0.1 * (1 - np.exp(-epoch/2)) + np.random.normal(0, 0.01)\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nFine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_practices = {\n",
    "    'Inference': [\n",
    "        'Save model config alongside weights',\n",
    "        'Standardize input data the same way as training',\n",
    "        'Use batch inference for efficiency',\n",
    "        'Consider model quantization for deployment'\n",
    "    ],\n",
    "    'Partial Fit': [\n",
    "        'Use lower learning rate (1/10 of original)',\n",
    "        'Train for fewer epochs',\n",
    "        'Mix old and new data to prevent forgetting',\n",
    "        'Monitor performance on old data'\n",
    "    ],\n",
    "    'Fine-Tuning': [\n",
    "        'Start by training only the head',\n",
    "        'Gradually unfreeze layers',\n",
    "        'Use discriminative learning rates',\n",
    "        'Early stopping to prevent overfitting'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Best Practices\")\n",
    "print(\"=\" * 60)\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for i, practice in enumerate(practices, 1):\n",
    "        print(f\"  {i}. {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "### Inference\n",
    "```python\n",
    "# Load configuration\n",
    "config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=loaded_config['n_vars'],\n",
    "    seq_len=loaded_config['seq_len'],\n",
    "    n_classes=loaded_config['n_classes']\n",
    ")\n",
    "\n",
    "# Standardize input\n",
    "X_std = tsai_rs.ts_standardize(X.astype(np.float32), by_sample=True)\n",
    "\n",
    "# Generate predictions\n",
    "probs, preds = model.predict(X_std)\n",
    "```\n",
    "\n",
    "### Partial Fit\n",
    "```python\n",
    "# Lower learning rate for incremental learning\n",
    "config = tsai_rs.LearnerConfig(lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Train for fewer epochs on new data\n",
    "```\n",
    "\n",
    "### Fine-Tuning\n",
    "```python\n",
    "# Two-phase fine-tuning:\n",
    "# 1. Train head with higher LR\n",
    "# 2. Train full model with lower LR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference\n",
    "print(\"Inference & Fine-Tuning Quick Reference\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n# Inference\")\n",
    "print(\"X_std = tsai_rs.ts_standardize(X.astype(np.float32), by_sample=True)\")\n",
    "print(\"\\n# Partial fit config\")\n",
    "print(\"config = tsai_rs.LearnerConfig(lr=1e-4)  # Lower LR\")\n",
    "print(\"\\n# Fine-tuning phases\")\n",
    "print(\"# Phase 1: Train head (lr=1e-3, freeze_backbone=True)\")\n",
    "print(\"# Phase 2: Full model (lr=1e-4, freeze_backbone=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up saved files\n",
    "import shutil\n",
    "\n",
    "if models_path.exists():\n",
    "    shutil.rmtree(models_path)\n",
    "    print(f\"Cleaned up {models_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
