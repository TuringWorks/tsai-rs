{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning for Time Series with tsai-rs\n",
    "\n",
    "This notebook demonstrates self-supervised pretraining concepts for time series using **tsai-rs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Self-supervised learning allows models to learn useful representations from unlabeled data. This is particularly valuable for time series when:\n",
    "\n",
    "1. **Limited labeled data**: Only a small fraction of data has labels\n",
    "2. **Expensive labeling**: Getting labels is costly or time-consuming\n",
    "3. **Transfer learning**: Pre-train on one domain, fine-tune on another\n",
    "\n",
    "Common self-supervised approaches for time series:\n",
    "- **MVP/TSBERT**: Mask-based pretraining (similar to BERT)\n",
    "- **Contrastive learning**: Learn to distinguish similar/dissimilar pairs\n",
    "- **Autoencoding**: Reconstruct input time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multivariate dataset\n",
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation for Self-Supervised Learning\n",
    "\n",
    "Self-supervised learning often relies on data augmentation to create different \"views\" of the same sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_view(X, seed=None):\n",
    "    \"\"\"Create an augmented view of the time series data.\"\"\"\n",
    "    X_aug = X.copy()\n",
    "    \n",
    "    # Apply random augmentations\n",
    "    X_aug = tsai_rs.add_gaussian_noise(X_aug, std=0.05, seed=seed)\n",
    "    X_aug = tsai_rs.mag_scale(X_aug, scale_range=(0.9, 1.1), seed=seed + 100 if seed else None)\n",
    "    \n",
    "    return X_aug\n",
    "\n",
    "# Create two views of the same sample\n",
    "sample = X_train_std[:1]\n",
    "view1 = create_augmented_view(sample, seed=42)\n",
    "view2 = create_augmented_view(sample, seed=123)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(sample[0, 0, :])\n",
    "axes[0].set_title('Original')\n",
    "\n",
    "axes[1].plot(view1[0, 0, :])\n",
    "axes[1].set_title('Augmented View 1')\n",
    "\n",
    "axes[2].plot(view2[0, 0, :])\n",
    "axes[2].set_title('Augmented View 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking for Self-Supervised Learning\n",
    "\n",
    "Similar to BERT, we can mask portions of the time series and train a model to reconstruct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_input(X, mask_ratio=0.15, seed=None):\n",
    "    \"\"\"Create masked version of time series.\n",
    "    \n",
    "    Args:\n",
    "        X: Input data (samples, vars, length)\n",
    "        mask_ratio: Proportion of timesteps to mask\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        X_masked: Masked input\n",
    "        mask: Boolean mask (True = masked)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n_samples, n_vars, seq_len = X.shape\n",
    "    n_mask = int(seq_len * mask_ratio)\n",
    "    \n",
    "    X_masked = X.copy()\n",
    "    masks = np.zeros((n_samples, seq_len), dtype=bool)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Select random positions to mask\n",
    "        mask_positions = np.random.choice(seq_len, n_mask, replace=False)\n",
    "        masks[i, mask_positions] = True\n",
    "        \n",
    "        # Set masked positions to 0 (or could use special token)\n",
    "        X_masked[i, :, mask_positions] = 0\n",
    "    \n",
    "    return X_masked, masks\n",
    "\n",
    "# Create masked version\n",
    "X_masked, masks = create_masked_input(X_train_std[:5], mask_ratio=0.2, seed=42)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "axes[0].plot(X_train_std[0, 0, :])\n",
    "axes[0].set_title('Original')\n",
    "\n",
    "axes[1].plot(X_masked[0, 0, :])\n",
    "# Mark masked positions\n",
    "masked_pos = np.where(masks[0])[0]\n",
    "axes[1].scatter(masked_pos, X_masked[0, 0, masked_pos], c='red', s=50, zorder=5, label='Masked')\n",
    "axes[1].set_title(f'Masked (20% = {len(masked_pos)} timesteps)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration for Self-Supervised Learning\n",
    "\n",
    "Models used in self-supervised pretraining typically have:\n",
    "1. An encoder that learns representations\n",
    "2. A projection head for the pretraining task\n",
    "3. After pretraining, replace projection head with classification/regression head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder configuration (for pretraining)\n",
    "# Use same architecture but with reconstruction head\n",
    "encoder_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=seq_len * n_vars  # Reconstruct all timesteps\n",
    ")\n",
    "print(f\"Encoder config (for reconstruction): {encoder_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After pretraining: use smaller head for classification\n",
    "classifier_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes  # Number of classes\n",
    ")\n",
    "print(f\"Classifier config: {classifier_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Limited Labels Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_limited_label_splits(y, label_fraction=0.1, seed=42):\n",
    "    \"\"\"Create train/valid splits with limited labels.\n",
    "    \n",
    "    Args:\n",
    "        y: Labels\n",
    "        label_fraction: Fraction of labeled training data\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        labeled_idx: Indices of labeled samples\n",
    "        unlabeled_idx: Indices of unlabeled samples\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_samples = len(y)\n",
    "    n_labeled = int(n_samples * label_fraction)\n",
    "    \n",
    "    # Stratified sampling to maintain class distribution\n",
    "    labeled_idx = []\n",
    "    classes = np.unique(y)\n",
    "    n_per_class = max(1, n_labeled // len(classes))\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        selected = np.random.choice(cls_idx, min(n_per_class, len(cls_idx)), replace=False)\n",
    "        labeled_idx.extend(selected)\n",
    "    \n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    unlabeled_idx = np.setdiff1d(np.arange(n_samples), labeled_idx)\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx\n",
    "\n",
    "# Create 10% labeled split\n",
    "labeled_idx, unlabeled_idx = create_limited_label_splits(y_train, label_fraction=0.1)\n",
    "\n",
    "print(f\"Total training samples: {len(y_train)}\")\n",
    "print(f\"Labeled samples (10%): {len(labeled_idx)}\")\n",
    "print(f\"Unlabeled samples: {len(unlabeled_idx)}\")\n",
    "\n",
    "# Class distribution in labeled subset\n",
    "classes, counts = np.unique(y_train[labeled_idx], return_counts=True)\n",
    "print(f\"\\nClass distribution in labeled subset:\")\n",
    "for c, cnt in zip(classes, counts):\n",
    "    print(f\"  Class {c}: {cnt} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Supervised Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_training_pipeline(X_train, y_train, X_test, y_test, label_fraction=0.1, seed=42):\n",
    "    \"\"\"Simulate self-supervised learning pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Split data into labeled/unlabeled\n",
    "    2. Pretrain on ALL data (no labels needed)\n",
    "    3. Fine-tune on labeled subset\n",
    "    \"\"\"\n",
    "    # Step 1: Create splits\n",
    "    labeled_idx, unlabeled_idx = create_limited_label_splits(y_train, label_fraction, seed)\n",
    "    \n",
    "    # Labeled and unlabeled data\n",
    "    X_labeled = X_train[labeled_idx]\n",
    "    y_labeled = y_train[labeled_idx]\n",
    "    X_unlabeled = X_train[unlabeled_idx]\n",
    "    \n",
    "    print(f\"Step 1: Data Split\")\n",
    "    print(f\"  Labeled samples: {len(X_labeled)}\")\n",
    "    print(f\"  Unlabeled samples: {len(X_unlabeled)}\")\n",
    "    \n",
    "    # Step 2: Pretrain on ALL data (both labeled and unlabeled)\n",
    "    print(f\"\\nStep 2: Pretraining on ALL {len(X_train)} samples\")\n",
    "    print(f\"  (Uses no labels - self-supervised)\")\n",
    "    \n",
    "    # During pretraining, would:\n",
    "    # - Mask random portions of time series\n",
    "    # - Train model to reconstruct masked portions\n",
    "    # - Or use contrastive learning with augmented views\n",
    "    \n",
    "    # Step 3: Fine-tune on labeled data only\n",
    "    print(f\"\\nStep 3: Fine-tuning on {len(X_labeled)} labeled samples\")\n",
    "    print(f\"  (Uses {label_fraction*100:.0f}% of labels)\")\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx\n",
    "\n",
    "# Run pipeline simulation\n",
    "labeled_idx, unlabeled_idx = ssl_training_pipeline(\n",
    "    X_train_std, y_train, X_test_std, y_test,\n",
    "    label_fraction=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Supervised vs Self-Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configurations for different label fractions\n",
    "label_fractions = [0.1, 0.25, 0.5, 1.0]\n",
    "\n",
    "print(f\"{'Label %':<10} {'Labeled':<10} {'Unlabeled':<12} {'SSL Benefit'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for frac in label_fractions:\n",
    "    labeled_idx, unlabeled_idx = create_limited_label_splits(y_train, frac)\n",
    "    n_labeled = len(labeled_idx)\n",
    "    n_unlabeled = len(unlabeled_idx)\n",
    "    \n",
    "    # SSL benefits more when labeled data is scarce\n",
    "    benefit = \"High\" if frac <= 0.1 else \"Medium\" if frac <= 0.5 else \"Lower\"\n",
    "    \n",
    "    print(f\"{frac*100:>6.0f}%    {n_labeled:<10} {n_unlabeled:<12} {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets for SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlabeled dataset (for pretraining)\n",
    "# Note: y is not used during pretraining\n",
    "X_pretrain = X_train_std  # All training data\n",
    "print(f\"Pretraining dataset: {X_pretrain.shape}\")\n",
    "\n",
    "# Labeled dataset (for fine-tuning)\n",
    "labeled_idx, _ = create_limited_label_splits(y_train, label_fraction=0.1)\n",
    "X_finetune = X_train_std[labeled_idx]\n",
    "y_finetune = y_train[labeled_idx]\n",
    "\n",
    "train_ds = tsai_rs.TSDataset(X_finetune, y_finetune)\n",
    "print(f\"Fine-tuning dataset: {train_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for different training scenarios\n",
    "\n",
    "# Pretraining: Larger model, more capacity\n",
    "pretrain_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    nf=64,  # More filters\n",
    "    depth=8  # Deeper network\n",
    ")\n",
    "\n",
    "# Fine-tuning: Use pretrained weights\n",
    "finetune_config = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    nf=64,\n",
    "    depth=8,\n",
    "    fc_dropout=0.3  # Add dropout for fine-tuning\n",
    ")\n",
    "\n",
    "print(f\"Pretrain config: {pretrain_config}\")\n",
    "print(f\"Fine-tune config: {finetune_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Schedule for SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining: Longer training, lower LR at end\n",
    "pretrain_epochs = 200\n",
    "pretrain_lr = 1e-2\n",
    "\n",
    "# Fine-tuning: Shorter training, lower LR\n",
    "finetune_epochs = 50\n",
    "finetune_lr = 1e-3  # Lower LR for fine-tuning\n",
    "\n",
    "# Calculate steps\n",
    "batch_size = 32\n",
    "n_pretrain = len(X_train_std)\n",
    "n_finetune = len(X_finetune)\n",
    "\n",
    "pretrain_steps = pretrain_epochs * ((n_pretrain + batch_size - 1) // batch_size)\n",
    "finetune_steps = finetune_epochs * ((n_finetune + batch_size - 1) // batch_size)\n",
    "\n",
    "pretrain_scheduler = tsai_rs.OneCycleLR.simple(max_lr=pretrain_lr, total_steps=pretrain_steps)\n",
    "finetune_scheduler = tsai_rs.OneCycleLR.simple(max_lr=finetune_lr, total_steps=finetune_steps)\n",
    "\n",
    "print(\"Self-Supervised Learning Schedule:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\\nPretraining:\")\n",
    "print(f\"  Epochs: {pretrain_epochs}\")\n",
    "print(f\"  Samples: {n_pretrain} (all data, no labels)\")\n",
    "print(f\"  LR: {pretrain_lr}\")\n",
    "print(f\"  Total steps: {pretrain_steps}\")\n",
    "\n",
    "print(f\"\\nFine-tuning:\")\n",
    "print(f\"  Epochs: {finetune_epochs}\")\n",
    "print(f\"  Samples: {n_finetune} (10% labeled)\")\n",
    "print(f\"  LR: {finetune_lr}\")\n",
    "print(f\"  Total steps: {finetune_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated self-supervised learning concepts for time series:\n",
    "\n",
    "### Key Concepts\n",
    "1. **Data Augmentation**: Create different views of the same sample\n",
    "2. **Masking**: Hide parts of the time series for reconstruction\n",
    "3. **Limited Labels**: Benefit most when labels are scarce\n",
    "\n",
    "### Self-Supervised Pipeline\n",
    "1. **Pretrain** on ALL data (no labels)\n",
    "2. **Fine-tune** on labeled subset\n",
    "\n",
    "### Benefits\n",
    "- Better performance with limited labels\n",
    "- Learns useful representations from unlabeled data\n",
    "- Can leverage large unlabeled datasets\n",
    "\n",
    "### tsai-rs Functions Used\n",
    "- `ts_standardize`: Normalize data\n",
    "- `add_gaussian_noise`: Data augmentation\n",
    "- `mag_scale`: Magnitude scaling augmentation\n",
    "- Model configs: `InceptionTimePlusConfig`, etc.\n",
    "- Training: `LearnerConfig`, `OneCycleLR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference for SSL with tsai-rs\n",
    "print(\"Self-Supervised Learning with tsai-rs:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\n1. Load and standardize data:\")\n",
    "print(\"   X_train, y_train, _, _ = tsai_rs.get_UCR_data(dsid)\")\n",
    "print(\"   X_std = tsai_rs.ts_standardize(X_train, by_sample=True)\")\n",
    "\n",
    "print(\"\\n2. Create augmented views:\")\n",
    "print(\"   X_aug = tsai_rs.add_gaussian_noise(X_std, std=0.05)\")\n",
    "print(\"   X_aug = tsai_rs.mag_scale(X_aug, scale_range=(0.9, 1.1))\")\n",
    "\n",
    "print(\"\\n3. Configure model:\")\n",
    "print(\"   config = tsai_rs.InceptionTimePlusConfig(...)\")\n",
    "\n",
    "print(\"\\n4. Pretrain (no labels):\")\n",
    "print(\"   # Train with reconstruction/contrastive loss\")\n",
    "print(\"   # Use all data\")\n",
    "\n",
    "print(\"\\n5. Fine-tune (with labels):\")\n",
    "print(\"   # Load pretrained weights\")\n",
    "print(\"   # Train classifier head on labeled data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
