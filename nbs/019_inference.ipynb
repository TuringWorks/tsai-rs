{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference",
    "",
    "> Model inference and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model_output):",
    "    \"\"\"Convert logits to probabilities using softmax.\"\"\"",
    "    exp_scores = np.exp(model_output - np.max(model_output, axis=-1, keepdims=True))",
    "    return exp_scores / exp_scores.sum(axis=-1, keepdims=True)",
    "",
    "def predict_class(proba):",
    "    \"\"\"Get predicted class from probabilities.\"\"\"",
    "    return np.argmax(proba, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example",
    "logits = np.array([[2.0, 1.0, 0.1], [0.5, 2.5, 1.0]])",
    "proba = predict_proba(logits)",
    "preds = predict_class(proba)",
    "print(f\"Probabilities:\\n{proba}\")",
    "print(f\"Predictions: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(X, batch_size=64):",
    "    \"\"\"Run inference in batches.\"\"\"",
    "    n_samples = len(X)",
    "    predictions = []",
    "    ",
    "    for start in range(0, n_samples, batch_size):",
    "        end = min(start + batch_size, n_samples)",
    "        batch = X[start:end]",
    "        # In practice, this would call the model",
    "        # pred = model.predict(batch)",
    "        # predictions.append(pred)",
    "    ",
    "    # return np.concatenate(predictions)",
    "    print(f\"Would process {n_samples} samples in {(n_samples + batch_size - 1) // batch_size} batches\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}