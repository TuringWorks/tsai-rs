{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/timeseriesAI/tsai-rs\" target=\"_parent\"><img src=\"https://img.shields.io/badge/tsai--rs-Time%20Series%20AI%20in%20Rust-blue\" alt=\"tsai-rs\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Architecture Comparison with tsai-rs\n",
    "\n",
    "This notebook compares different deep learning architectures for time series classification using **tsai-rs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "tsai-rs provides multiple state-of-the-art architectures:\n",
    "\n",
    "1. **InceptionTimePlus** - CNN-based with multi-scale convolutions\n",
    "2. **ResNetPlus** - Deep residual networks\n",
    "3. **PatchTST** - Transformer with patching\n",
    "4. **TST** - Time Series Transformer\n",
    "5. **RNNPlus** - LSTM/GRU based models\n",
    "6. **MiniRocket** - Fast feature extraction with linear classifier\n",
    "\n",
    "This notebook shows how to configure each architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install tsai-rs\n",
    "\n",
    "```bash\n",
    "cd crates/tsai_python\n",
    "maturin develop --release\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsai_rs\n",
    "import numpy as np\n",
    "\n",
    "print(f\"tsai-rs version: {tsai_rs.version()}\")\n",
    "tsai_rs.my_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a multivariate dataset\n",
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "# Get data dimensions\n",
    "n_samples = X_train.shape[0]\n",
    "n_vars = X_train.shape[1]\n",
    "seq_len = X_train.shape[2]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(f\"Dataset: {dsid}\")\n",
    "print(f\"Train samples: {n_samples}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Variables: {n_vars}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Classes: {n_classes}\")\n",
    "print(f\"Class labels: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = tsai_rs.ts_standardize(X_train.astype(np.float32), by_sample=True)\n",
    "X_test_std = tsai_rs.ts_standardize(X_test.astype(np.float32), by_sample=True)\n",
    "\n",
    "print(f\"Standardized data shape: {X_train_std.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 1: InceptionTimePlus\n",
    "\n",
    "InceptionTime is a CNN-based architecture that uses inception modules with multi-scale convolutions. It's fast, accurate, and works well on most datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default configuration\n",
    "inception_default = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "print(f\"InceptionTimePlus (default): {inception_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom configuration with more filters\n",
    "inception_large = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    nf=64,        # Number of filters (default: 32)\n",
    "    depth=8       # Number of inception modules (default: 6)\n",
    ")\n",
    "print(f\"InceptionTimePlus (large): {inception_large}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dropout for regularization\n",
    "inception_dropout = tsai_rs.InceptionTimePlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    nf=32,\n",
    "    fc_dropout=0.3  # Dropout in final classifier\n",
    ")\n",
    "print(f\"InceptionTimePlus (with dropout): {inception_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 2: ResNetPlus\n",
    "\n",
    "ResNet uses residual connections to train deeper networks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default ResNet configuration\n",
    "resnet_default = tsai_rs.ResNetPlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "print(f\"ResNetPlus (default): {resnet_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper ResNet\n",
    "resnet_deep = tsai_rs.ResNetPlusConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    nf=128  # More filters\n",
    ")\n",
    "print(f\"ResNetPlus (deep): {resnet_deep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 3: PatchTST\n",
    "\n",
    "PatchTST applies the transformer architecture to time series by dividing the sequence into patches, similar to Vision Transformer (ViT) for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default PatchTST configuration\n",
    "patchtst_default = tsai_rs.PatchTSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    patch_len=8,      # Length of each patch\n",
    "    stride=8          # Stride between patches\n",
    ")\n",
    "print(f\"PatchTST (default): {patchtst_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger PatchTST with more attention heads\n",
    "patchtst_large = tsai_rs.PatchTSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    patch_len=16,\n",
    "    stride=8,          # Overlapping patches\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    d_ff=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "print(f\"PatchTST (large): {patchtst_large}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 4: TST (Time Series Transformer)\n",
    "\n",
    "TST is a pure transformer architecture for time series that processes each timestep as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default TST configuration\n",
    "tst_default = tsai_rs.TSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "print(f\"TST (default): {tst_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom TST with specific hyperparameters\n",
    "tst_custom = tsai_rs.TSTConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes,\n",
    "    d_model=64,       # Model dimension\n",
    "    n_heads=4,        # Number of attention heads\n",
    "    n_layers=3,       # Number of transformer layers\n",
    "    d_ff=128,         # Feed-forward dimension\n",
    "    dropout=0.1,      # Dropout rate\n",
    "    fc_dropout=0.2    # Final classifier dropout\n",
    ")\n",
    "print(f\"TST (custom): {tst_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 5: RNNPlus\n",
    "\n",
    "RNNPlus provides LSTM and GRU based architectures, which are effective for sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LSTM configuration\nlstm_config = tsai_rs.RNNPlusConfig(\n    n_vars=n_vars,\n    seq_len=seq_len,\n    n_classes=n_classes,\n    rnn_type='lstm',\n    hidden_size=64,\n    n_layers=2,\n    bidirectional=True,\n    dropout=0.2\n)\nprint(f\"LSTM: {lstm_config}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GRU configuration\ngru_config = tsai_rs.RNNPlusConfig(\n    n_vars=n_vars,\n    seq_len=seq_len,\n    n_classes=n_classes,\n    rnn_type='gru',\n    hidden_size=128,\n    n_layers=3,\n    bidirectional=True,\n    dropout=0.3\n)\nprint(f\"GRU: {gru_config}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Unidirectional RNN (for real-time/streaming applications)\nrnn_uni = tsai_rs.RNNPlusConfig(\n    n_vars=n_vars,\n    seq_len=seq_len,\n    n_classes=n_classes,\n    rnn_type='lstm',\n    hidden_size=64,\n    n_layers=2,\n    bidirectional=False  # One direction only\n)\nprint(f\"Unidirectional LSTM: {rnn_uni}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 6: MiniRocket\n",
    "\n",
    "MiniRocket is a feature extraction method that transforms time series into fixed-length feature vectors, which are then classified using a linear model. It's extremely fast while maintaining competitive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default MiniRocket configuration\n",
    "minirocket_default = tsai_rs.MiniRocketConfig(\n",
    "    n_vars=n_vars,\n",
    "    seq_len=seq_len,\n",
    "    n_classes=n_classes\n",
    ")\n",
    "print(f\"MiniRocket (default): {minirocket_default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MiniRocket with more features\nminirocket_large = tsai_rs.MiniRocketConfig(\n    n_vars=n_vars,\n    seq_len=seq_len,\n    n_classes=n_classes,\n    n_features=10000   # More features for potentially better accuracy\n)\nprint(f\"MiniRocket (large): {minirocket_large}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    ('InceptionTimePlus', inception_default, 'CNN', 'Multi-scale convolutions'),\n",
    "    ('ResNetPlus', resnet_default, 'CNN', 'Residual connections'),\n",
    "    ('PatchTST', patchtst_default, 'Transformer', 'Patching + attention'),\n",
    "    ('TST', tst_default, 'Transformer', 'Pure attention'),\n",
    "    ('RNNPlus (LSTM)', lstm_config, 'RNN', 'Sequential processing'),\n",
    "    ('RNNPlus (GRU)', gru_config, 'RNN', 'Gated units'),\n",
    "    ('MiniRocket', minirocket_default, 'Feature', 'Random kernels + linear'),\n",
    "]\n",
    "\n",
    "print(f\"{'Architecture':<20} {'Type':<12} {'Description':<30}\")\n",
    "print(\"-\" * 65)\n",
    "for name, config, arch_type, desc in architectures:\n",
    "    print(f\"{name:<20} {arch_type:<12} {desc:<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Architecture\n",
    "\n",
    "| Use Case | Recommended Architecture |\n",
    "|----------|-------------------------|\n",
    "| General purpose, fast training | InceptionTimePlus |\n",
    "| Very deep networks | ResNetPlus |\n",
    "| Long sequences | PatchTST |\n",
    "| Capturing long-range dependencies | TST, PatchTST |\n",
    "| Streaming/real-time | RNNPlus (unidirectional) |\n",
    "| Very fast training needed | MiniRocket |\n",
    "| Small datasets | MiniRocket, InceptionTimePlus |\n",
    "| Large datasets | Any (try PatchTST/TST) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common learner configuration\n",
    "learner_config = tsai_rs.LearnerConfig(\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "print(f\"Learner config: {learner_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-cycle learning rate scheduler\n",
    "n_epochs = 25\n",
    "batch_size = 32\n",
    "steps_per_epoch = (n_samples + batch_size - 1) // batch_size\n",
    "total_steps = n_epochs * steps_per_epoch\n",
    "\n",
    "scheduler = tsai_rs.OneCycleLR.simple(max_lr=1e-3, total_steps=total_steps)\n",
    "print(f\"OneCycleLR scheduler: {scheduler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Setup Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_configs_for_dataset(X, y):\n    \"\"\"Create all architecture configs for a given dataset.\"\"\"\n    n_vars = X.shape[1]\n    seq_len = X.shape[2]\n    n_classes = len(np.unique(y))\n    \n    configs = {\n        'InceptionTimePlus': tsai_rs.InceptionTimePlusConfig(\n            n_vars=n_vars, seq_len=seq_len, n_classes=n_classes\n        ),\n        'ResNetPlus': tsai_rs.ResNetPlusConfig(\n            n_vars=n_vars, seq_len=seq_len, n_classes=n_classes\n        ),\n        'PatchTST': tsai_rs.PatchTSTConfig.for_classification(\n            n_vars=n_vars, seq_len=seq_len, n_classes=n_classes\n        ),\n        'TST': tsai_rs.TSTConfig(\n            n_vars=n_vars, seq_len=seq_len, n_classes=n_classes,\n            d_model=64, n_heads=4, n_layers=2\n        ),\n        'LSTM': tsai_rs.RNNPlusConfig(\n            n_vars=n_vars, seq_len=seq_len, n_classes=n_classes,\n            rnn_type='lstm', hidden_size=64, n_layers=2, bidirectional=True\n        ),\n        'MiniRocket': tsai_rs.MiniRocketConfig(\n            n_vars=n_vars, seq_len=seq_len, n_classes=n_classes\n        ),\n    }\n    return configs\n\n# Create configs for our dataset\nconfigs = create_configs_for_dataset(X_train, y_train)\n\nprint(\"Architecture configurations for NATOPS dataset:\")\nprint(\"=\" * 50)\nfor name, config in configs.items():\n    print(f\"\\n{name}:\")\n    print(f\"  {config}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['ECG200', 'GunPoint', 'FordA', 'Wafer', 'NATOPS']\n",
    "\n",
    "print(f\"{'Dataset':<15} {'Train':<8} {'Test':<8} {'Vars':<6} {'Len':<8} {'Classes':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dsid in datasets:\n",
    "    try:\n",
    "        X_train, y_train, X_test, y_test = tsai_rs.get_UCR_data(dsid, return_split=True)\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        \n",
    "        print(f\"{dsid:<15} {X_train.shape[0]:<8} {X_test.shape[0]:<8} \"\n",
    "              f\"{X_train.shape[1]:<6} {X_train.shape[2]:<8} {n_classes:<8}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{dsid:<15} Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the architecture options in tsai-rs:\n",
    "\n",
    "### CNN-based\n",
    "- **InceptionTimePlus**: Multi-scale convolutions, fast and accurate\n",
    "- **ResNetPlus**: Deep residual networks\n",
    "\n",
    "### Transformer-based\n",
    "- **PatchTST**: Patches + transformer, good for long sequences\n",
    "- **TST**: Pure transformer, captures long-range dependencies\n",
    "\n",
    "### RNN-based\n",
    "- **RNNPlus**: LSTM/GRU, good for sequential patterns\n",
    "\n",
    "### Feature-based\n",
    "- **MiniRocket**: Extremely fast, random kernel features\n",
    "\n",
    "### Key Configurations\n",
    "All architectures share common parameters:\n",
    "- `n_vars`: Number of input variables\n",
    "- `seq_len`: Sequence length\n",
    "- `n_classes`: Number of output classes\n",
    "\n",
    "Architecture-specific parameters allow fine-tuning for specific datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick reference: creating configs\nn_vars, seq_len, n_classes = 24, 51, 6\n\nprint(\"Quick reference for creating configs:\")\nprint(\"=\" * 50)\nprint(f\"\\nInceptionTimePlus:\")\nprint(f\"  tsai_rs.InceptionTimePlusConfig(n_vars={n_vars}, seq_len={seq_len}, n_classes={n_classes})\")\nprint(f\"\\nResNetPlus:\")\nprint(f\"  tsai_rs.ResNetPlusConfig(n_vars={n_vars}, seq_len={seq_len}, n_classes={n_classes})\")\nprint(f\"\\nPatchTST:\")\nprint(f\"  tsai_rs.PatchTSTConfig.for_classification(n_vars={n_vars}, seq_len={seq_len}, n_classes={n_classes})\")\nprint(f\"\\nTST:\")\nprint(f\"  tsai_rs.TSTConfig(n_vars={n_vars}, seq_len={seq_len}, n_classes={n_classes}, d_model=64, n_heads=4)\")\nprint(f\"\\nRNNPlus:\")\nprint(f\"  tsai_rs.RNNPlusConfig(n_vars={n_vars}, seq_len={seq_len}, n_classes={n_classes}, rnn_type='lstm')\")\nprint(f\"\\nMiniRocket:\")\nprint(f\"  tsai_rs.MiniRocketConfig(n_vars={n_vars}, seq_len={seq_len}, n_classes={n_classes})\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}